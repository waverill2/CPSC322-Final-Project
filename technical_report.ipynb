{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Josh Seward & Weston Averill  \n",
    "5/4/2022  \n",
    "CPSC322, Spring 2022  \n",
    "MLB All-Star Predictor\n",
    "\n",
    "# MLB All-Star Predictor Final Project Technical Report\n",
    "\n",
    "### Introduction\n",
    "\n",
    "For this project, we used data from Kaggle's \"History of Baseball\" dataset where we selected three different files of data for analysis. The goal was to use our classifiers with binary classification and attempt to predict whether or not an unseen instance is an MLB all-star. After puting the data through our dummy, naive-bayes, and random forest classifiers, we learned *TODO: insert best classifier* performed the best. \n",
    "\n",
    "### Data Analysis\n",
    "\n",
    "The three main data files used for this project were 'Allstar.txt', 'Batting.txt', and 'Fielding.txt'. The first step of the data cleaning was to join the batting stats and fielding stats so that each instance(row) in the data would represent one players stats. To do this, we performed an inner join on these two datasets with the key being playerID and yearID. We thought this would correctly give us one instance for each player for each year they played, but we were still getting multiple rows for a player for the same year. The reason this was happening was because the dataset had instances for each of the teams they played on. For example, if a player played for three teams in 2015, after our join, our table would have three instaces with year 2015, one for each team. To solve this problem, we added each corresponding attribute value together to create one instace, that now represents the players' stats for that year, which is what we wanted. \n",
    "\n",
    "After getting player stats for each specific year, we used the 'Allstar.txt' data to add a class label to each of our instances. If a playerID and yearID in our data matched to an instance in the Allstar data, the class label would be set to true, false otherwise. \n",
    "\n",
    "After doing the previous steps, we now have clean data for MLB players from 2000-2015 that represents their stats, and their all-star status. In baseball, the number of non all-stars is much greater than the number of all-stars. In order to be able to actually see how well our classifiers make predicions, we had to down-sample the amount of non all-stars from about 19,000 to around 500, which is the same number of all-stars in the dataset. This gave us an even distribution of class labels. \n",
    "\n",
    "Our final data file has 16 attributes with 'class' being used as the class label. Joining the datasets gave us many attributes, but we decided to only include attributes that were the most useful and common in baseball. \n",
    "\n",
    "The below code cells implement steps described to clean the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "import os\n",
    "import copy\n",
    "import mysklearn.myutils as myutils\n",
    "import random\n",
    "importlib.reload(myutils)\n",
    "# uncomment once you paste your mypytable.py into mysklearn package\n",
    "import mysklearn.mypytable as mypytable\n",
    "importlib.reload(mypytable)\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "# put data into a table\n",
    "path = os.path.join(\"output_data\", \"batting_fielding_joined.txt\")\n",
    "batting_fielding_joined = mypytable.MyPyTable().load_from_file(path)\n",
    "batting_fielding_joined.drop_column(\"playerID\")\n",
    "table_copy = copy.deepcopy(batting_fielding_joined.data)\n",
    "# print(len(table_copy))\n",
    "# batting_fielding_joined.drop_column('GZ')\n",
    "# print(len(batting_fielding_joined.column_names))\n",
    "# batting_fielding_joined.drop_column('GS')\n",
    "\n",
    "# column = batting_fielding_joined.get_column('E', True)\n",
    "# cutoffs = myutils.compute_equal_width_cutoffs(column, 8)\n",
    "# print(cutoffs)\n",
    "# new_list = []\n",
    "# for value in column:\n",
    "#     if value == max(column):\n",
    "#         new_list.append(8)\n",
    "#     else:\n",
    "#         for i in range(len(cutoffs)-1):\n",
    "#             if cutoffs[i] <= value < cutoffs[i + 1]:\n",
    "#                 new_list.append(i+1)\n",
    "\n",
    "# print(new_list)\n",
    "# for i, row in enumerate(table_copy):\n",
    "#     # row[17] = str(new_list[i])\n",
    "#     row[1] = str(row[1])\n",
    "# batting_fielding_joined.data = table_copy\n",
    "\n",
    "for row in table_copy:\n",
    "    for i in range(len(row)):\n",
    "        row[i] = str(row[i])\n",
    "# print(table_copy)\n",
    "# batting_fielding_joined.data = table_copy\n",
    "# batting_fielding_joined.save_to_file(path)\n",
    "# print(table_copy)\n",
    "\n",
    "rows_of_allstar = []\n",
    "rows_of_nonallstar = []\n",
    "for row in table_copy:\n",
    "    if row[-1] == \"true\":\n",
    "        rows_of_allstar.append(row)\n",
    "    else:\n",
    "        rows_of_nonallstar.append(row)\n",
    "# print(len(rows_of_allstar), len(rows_of_nonallstar))\n",
    "# print(rows_of_nonallstar[1])\n",
    "random.shuffle(rows_of_nonallstar)\n",
    "# print(rows_of_nonallstar[1])\n",
    "new_non_allstar = []\n",
    "for i in range(524):\n",
    "    new_non_allstar.append(rows_of_nonallstar[i])\n",
    "table_copy = rows_of_allstar + new_non_allstar\n",
    "random.shuffle(table_copy)\n",
    "\n",
    "\n",
    "# take the classes col from table and put into y table\n",
    "y = []\n",
    "for instance in table_copy:\n",
    "    y.append(instance[-1])\n",
    "    del(instance[-1])\n",
    "\n",
    "# # create another copy of the dataset\n",
    "table_copy_classifier = copy.deepcopy(table_copy)\n",
    "\n",
    "# # get train and test indices\n",
    "train_indices, test_indices = myevaluation.stratified_kfold_cross_validation(table_copy, y, 10, 0, False)\n",
    "new_table = []\n",
    "indices_to_train = []\n",
    "indices_to_test = []\n",
    "y_train = []\n",
    "predictions = []\n",
    "for i in range(len(train_indices)):\n",
    "    for index in train_indices[i]:\n",
    "        indices_to_train.append(table_copy_classifier[index])\n",
    "        y_train.append(y[index])\n",
    "    for index in test_indices[i]:\n",
    "        indices_to_test.append(table_copy_classifier[index])\n",
    "    dummy_classifier = MyDummyClassifier()\n",
    "    dummy_classifier.fit(indices_to_train, y_train)\n",
    "    predictions.append(dummy_classifier.predict(indices_to_test))\n",
    "    indices_to_train = []\n",
    "    indices_to_test = []\n",
    "    y_train = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for instance in test_indices:\n",
    "    for index in instance:\n",
    "        y_true.append(y[index])\n",
    "for instance in predictions:\n",
    "    for prediction in instance:\n",
    "        y_pred.append(prediction)\n",
    "\n",
    "accuracy = myevaluation.accuracy_score(y_true, y_pred, True)\n",
    "precision = myevaluation.binary_precision_score(y_true, y_pred, [\"false\", \"true\"], \"true\")\n",
    "recall = myevaluation.binary_recall_score(y_true, y_pred, [\"false\", \"true\"], \"true\")\n",
    "f1 = myevaluation.binary_f1_score(y_true, y_pred, [\"false\", \"true\"], \"true\")\n",
    "matrix = myevaluation.confusion_matrix(y_true, y_pred, [\"false\", \"true\"])\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "# print(f'Error Rate: {1-accuracy}')\n",
    "# print(f'Precision: {precision}')\n",
    "# print(f'Recall: {recall}')\n",
    "# print(f'F1 measure: {f1}')\n",
    "# print(f'Confusion Matrix: {matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "import os\n",
    "import copy\n",
    "import mysklearn.myutils as myutils\n",
    "import random\n",
    "importlib.reload(myutils)\n",
    "# uncomment once you paste your mypytable.py into mysklearn package\n",
    "import mysklearn.mypytable as mypytable\n",
    "importlib.reload(mypytable)\n",
    "\n",
    "# uncomment once you paste your myclassifiers.py into mysklearn package\n",
    "import mysklearn.myclassifiers as myclassifiers\n",
    "importlib.reload(myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier, MyDecisionTreeClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "# put data into a table\n",
    "path = os.path.join(\"output_data\", \"batting_fielding_joined.txt\")\n",
    "batting_fielding_joined = mypytable.MyPyTable().load_from_file(path)\n",
    "batting_fielding_joined.drop_column(\"playerID\")\n",
    "table_copy = copy.deepcopy(batting_fielding_joined.data)\n",
    "# print(len(table_copy))\n",
    "# batting_fielding_joined.drop_column('GZ')\n",
    "# print(len(batting_fielding_joined.column_names))\n",
    "# batting_fielding_joined.drop_column('GS')\n",
    "\n",
    "# column = batting_fielding_joined.get_column('E', True)\n",
    "# cutoffs = myutils.compute_equal_width_cutoffs(column, 8)\n",
    "# print(cutoffs)\n",
    "# new_list = []\n",
    "# for value in column:\n",
    "#     if value == max(column):\n",
    "#         new_list.append(8)\n",
    "#     else:\n",
    "#         for i in range(len(cutoffs)-1):\n",
    "#             if cutoffs[i] <= value < cutoffs[i + 1]:\n",
    "#                 new_list.append(i+1)\n",
    "\n",
    "# print(new_list)\n",
    "# for i, row in enumerate(table_copy):\n",
    "#     # row[17] = str(new_list[i])\n",
    "#     row[1] = str(row[1])\n",
    "# batting_fielding_joined.data = table_copy\n",
    "\n",
    "for row in table_copy:\n",
    "    for i in range(len(row)):\n",
    "        row[i] = str(row[i])\n",
    "# print(table_copy)\n",
    "# batting_fielding_joined.data = table_copy\n",
    "# batting_fielding_joined.save_to_file(path)\n",
    "# print(table_copy)\n",
    "\n",
    "rows_of_allstar = []\n",
    "rows_of_nonallstar = []\n",
    "for row in table_copy:\n",
    "    if row[-1] == \"true\":\n",
    "        rows_of_allstar.append(row)\n",
    "    else:\n",
    "        rows_of_nonallstar.append(row)\n",
    "# print(len(rows_of_allstar), len(rows_of_nonallstar))\n",
    "# print(rows_of_nonallstar[1])\n",
    "random.shuffle(rows_of_nonallstar)\n",
    "# print(rows_of_nonallstar[1])\n",
    "new_non_allstar = []\n",
    "for i in range(524):\n",
    "    new_non_allstar.append(rows_of_nonallstar[i])\n",
    "table_copy = rows_of_allstar + new_non_allstar\n",
    "random.shuffle(table_copy)\n",
    "\n",
    "\n",
    "# take the classes col from table and put into y table\n",
    "y = []\n",
    "for instance in table_copy:\n",
    "    y.append(instance[-1])\n",
    "    del(instance[-1])\n",
    "\n",
    "# # create another copy of the dataset\n",
    "table_copy_classifier = copy.deepcopy(table_copy)\n",
    "\n",
    "# # get train and test indices\n",
    "train_indices, test_indices = myevaluation.stratified_kfold_cross_validation(table_copy, y, 10, 0, False)\n",
    "new_table = []\n",
    "indices_to_train = []\n",
    "indices_to_test = []\n",
    "y_train = []\n",
    "predictions = []\n",
    "for i in range(len(train_indices)):\n",
    "    for index in train_indices[i]:\n",
    "        indices_to_train.append(table_copy_classifier[index])\n",
    "        y_train.append(y[index])\n",
    "    for index in test_indices[i]:\n",
    "        indices_to_test.append(table_copy_classifier[index])\n",
    "    tree_classifier = MyDecisionTreeClassifier()\n",
    "    tree_classifier.fit(indices_to_train, y_train)\n",
    "    predictions.append(tree_classifier.predict(indices_to_test))\n",
    "    indices_to_train = []\n",
    "    indices_to_test = []\n",
    "    y_train = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for instance in test_indices:\n",
    "    for index in instance:\n",
    "        y_true.append(y[index])\n",
    "for instance in predictions:\n",
    "    for prediction in instance:\n",
    "        y_pred.append(prediction)\n",
    "\n",
    "accuracy = myevaluation.accuracy_score(y_true, y_pred, True)\n",
    "precision = myevaluation.binary_precision_score(y_true, y_pred, [\"false\", \"true\"], \"true\")\n",
    "recall = myevaluation.binary_recall_score(y_true, y_pred, [\"false\", \"true\"], \"true\")\n",
    "f1 = myevaluation.binary_f1_score(y_true, y_pred, [\"false\", \"true\"], \"true\")\n",
    "matrix = myevaluation.confusion_matrix(y_true, y_pred, [\"false\", \"true\"])\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "# print(f'Error Rate: {1-accuracy}')\n",
    "# print(f'Precision: {precision}')\n",
    "# print(f'Recall: {recall}')\n",
    "# print(f'F1 measure: {f1}')\n",
    "# print(f'Confusion Matrix: {matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import copy\n",
    "importlib.reload(myutils)\n",
    "from mysklearn.mypytable import MyPyTable\n",
    "\n",
    "fname = os.path.join(\"input_data\", \"AllstarFull.csv\")\n",
    "allstar_table = MyPyTable().load_from_file(fname)\n",
    "# print(allstar_table.data)\n",
    "\n",
    "fname = os.path.join(\"input_data\", \"Batting.csv\")\n",
    "# batting_table = MyPyTable().load_from_file(fname)\n",
    "# print(batting_table.data)\n",
    "\n",
    "fname = os.path.join(\"input_data\", \"Fielding.csv\")\n",
    "# fielding_table = MyPyTable().load_from_file(fname)\n",
    "# print(fielding_table)\n",
    "\n",
    "fname = os.path.join(\"input_data\", \"player.csv\")\n",
    "player_table = MyPyTable().load_from_file(fname)\n",
    "# print(fielding_table)\n",
    "\n",
    "# ''' first drop rows from table if they are not in year 2000-2015 '''\n",
    "# use this code for all three datasets\n",
    "# lst_of_indices = [i for i in range(3916)]\n",
    "# allstar_table.drop_rows(lst_of_indices)\n",
    "# allstar_table_clean = os.path.join(\"output_data\", \"Allstar.txt\")\n",
    "# allstar_table.save_to_file(allstar_table_clean)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import copy\n",
    "importlib.reload(myutils)\n",
    "importlib.reload(mypytable)\n",
    "\n",
    "header = ['att1', 'att2']\n",
    "data = [['weston', 1], \n",
    "        ['mike', 2]]\n",
    "\n",
    "header2 = ['att1', 'att4', 'att5', 'att6']\n",
    "data2 = [['weston', 2000,3, 30],\n",
    "         ['weston', 2000, 3, 40]]\n",
    "\n",
    "table1 = mypytable.MyPyTable(header, data)\n",
    "table2 = mypytable.MyPyTable(header2, data2)\n",
    "\n",
    "new_table = []\n",
    "primarykey_list = []\n",
    "for row in table2.data:\n",
    "    key = str(row[0]) + str(row[1])\n",
    "    if key not in primarykey_list:\n",
    "        primarykey_list.append(key)\n",
    "# print(primarykey_list)\n",
    "\n",
    "for key in primarykey_list:\n",
    "    temp_table = []\n",
    "    for instance in table2.data:\n",
    "        temp_key = str(instance[0]) + str(instance[1])\n",
    "        if temp_key == key:\n",
    "            temp_table.append(instance)\n",
    "    if len(temp_table) == 1:\n",
    "        # add a zero to missing values\n",
    "        for i in range(2, len(temp_table[0])):\n",
    "            if type(temp_table[0][i]) != float and type(temp_table[0][i]) != int:\n",
    "                temp_table[0][i] = 0.0\n",
    "        new_table.append(temp_table[0])\n",
    "    else:\n",
    "        # print(temp_table)\n",
    "        new_row = temp_table[0]\n",
    "        for i in range(2, len(new_row)):\n",
    "            if type(new_row[i]) != float and type(new_row[i]) != int:\n",
    "                new_row[i] = 0.0\n",
    "        for i in range(1, len(temp_table)):\n",
    "                for j in range(2, len(temp_table[i])):\n",
    "                    if type(temp_table[i][j]) == float or type(temp_table[i][j]) == int:\n",
    "                        new_row[j] += temp_table[i][j]\n",
    "        new_table.append(new_row)\n",
    "table2.data = new_table\n",
    "# print(table2.data)\n",
    "            \n",
    "        \n",
    "# joined_table = table1.perform_inner_join(table2, [\"att1\"])\n",
    "# print(joined_table.data)\n",
    "# print(joined_table.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean the batting and fielding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import copy\n",
    "importlib.reload(myutils)\n",
    "importlib.reload(mypytable)\n",
    "\n",
    "path = os.path.join(\"output_data\", \"Batting.txt\")\n",
    "batting_table = mypytable.MyPyTable().load_from_file(path)\n",
    "\n",
    "path = os.path.join(\"output_data\", \"Fielding.txt\")\n",
    "fielding_table = mypytable.MyPyTable().load_from_file(path)\n",
    "\n",
    "\n",
    "# new_table = []\n",
    "# primarykey_list = []\n",
    "# for row in fielding_table.data:\n",
    "#     key = str(row[0]) + str(row[1])\n",
    "#     if key not in primarykey_list:\n",
    "#         primarykey_list.append(key)\n",
    "# # print(primarykey_list)\n",
    "\n",
    "# for key in primarykey_list:\n",
    "#     temp_table = []\n",
    "#     for instance in fielding_table.data:\n",
    "#         temp_key = str(instance[0]) + str(instance[1])\n",
    "#         if temp_key == key:\n",
    "#             temp_table.append(instance)\n",
    "#     if len(temp_table) == 1:\n",
    "#         # add a zero to missing values\n",
    "#         for i in range(2, len(temp_table[0])):\n",
    "#             if type(temp_table[0][i]) != float and type(temp_table[0][i]) != int:\n",
    "#                 temp_table[0][i] = 0.0\n",
    "#         new_table.append(temp_table[0])\n",
    "#     else:\n",
    "#         # print(temp_table)\n",
    "#         new_row = temp_table[0]\n",
    "#         for i in range(2, len(new_row)):\n",
    "#             if type(new_row[i]) != float and type(new_row[i]) != int:\n",
    "#                 new_row[i] = 0.0\n",
    "#         for i in range(1, len(temp_table)):\n",
    "#                 for j in range(2, len(temp_table[i])):\n",
    "#                     if type(temp_table[i][j]) == float or type(temp_table[i][j]) == int:\n",
    "#                         new_row[j] += temp_table[i][j]\n",
    "#         new_table.append(new_row)\n",
    "# fielding_table.data = new_table\n",
    "# path = os.path.join(\"output_data\", \"Fielding_clean.txt\")\n",
    "# fielding_table.save_to_file(path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import copy\n",
    "importlib.reload(myutils)\n",
    "importlib.reload(mypytable)\n",
    "\n",
    "fname = os.path.join(\"input_data\", \"player.csv\")\n",
    "player_table = MyPyTable().load_from_file(fname)\n",
    "# print(len(player_table.data))\n",
    "indices_to_remove = []\n",
    "# for i, instance in enumerate(player_table.data):\n",
    "#     if int(instance[1]) < 1955:\n",
    "#         indices_to_remove.append(i)\n",
    "# player_table.drop_rows(indices_to_remove)\n",
    "i = 0\n",
    "for i, instance in enumerate(player_table.data):\n",
    "    try:\n",
    "        if instance[1] < 1955:\n",
    "            indices_to_remove.append(i)\n",
    "    except:\n",
    "        indices_to_remove.append(i)\n",
    "# print(len(indices_to_remove))\n",
    "# player_table.drop_rows(indices_to_remove)\n",
    "# print(len(player_table.data))\n",
    "# player_clean = os.path.join(\"output_data\", \"player.txt\")\n",
    "# player_table.save_to_file(player_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are steps for joining the tables from output_data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import copy\n",
    "importlib.reload(myutils)\n",
    "importlib.reload(mypytable)\n",
    "\n",
    "# path = os.path.join(\"output_data\", \"Batting_clean.txt\")\n",
    "# batting_table = mypytable.MyPyTable().load_from_file(path)\n",
    "\n",
    "# path = os.path.join(\"output_data\", \"Fielding_clean.txt\")\n",
    "# fielding_table = mypytable.MyPyTable().load_from_file(path)\n",
    "\n",
    "# path = os.path.join(\"output_data\", \"player.txt\")\n",
    "# player_table = mypytable.MyPyTable().load_from_file(path)\n",
    "# print(batting_table.column_names)\n",
    "# print(fielding_table.column_names)\n",
    "# print(player_table.column_names)\n",
    "\n",
    "# going to drop some columns in the tables to try and speed up joins\n",
    "# batting_table.drop_column(\"stint\")\n",
    "# batting_table.drop_column(\"teamID\")\n",
    "# batting_table.drop_column(\"CS\")\n",
    "# batting_table.drop_column(\"SH\")\n",
    "# batting_table.drop_column(\"SF\")\n",
    "# batting_table.drop_column(\"GIDP\")\n",
    "# batting_table.drop_column(\"IBB\")\n",
    "# batting_table.drop_column(\"HBP\")\n",
    "# batting_table.drop_column(\"lgID\")\n",
    "# print(batting_table.column_names)\n",
    "\n",
    "# fielding_table.drop_column(\"stint\")\n",
    "# fielding_table.drop_column(\"teamID\")\n",
    "# fielding_table.drop_column(\"lgID\")\n",
    "\n",
    "# fielding_table.drop_column(\"SB\")\n",
    "# fielding_table.drop_column(\"CS\")\n",
    "# fielding_table.drop_column(\"ZR\")\n",
    "\n",
    "# fielding_table.drop_column(\"InnOuts\")\n",
    "# fielding_table.drop_column(\"POS\")\n",
    "# fielding_table.drop_column(\"PB\")\n",
    "\n",
    "# fielding_table.drop_column(\"WP\")\n",
    "# fielding_table.drop_column(\"DP\")\n",
    "# print(fielding_table.column_names)\n",
    "\n",
    "# player_table.drop_column(\"birth_month\")\n",
    "# player_table.drop_column(\"birth_day\")\n",
    "# player_table.drop_column(\"birth_country\")\n",
    "\n",
    "# player_table.drop_column(\"birth_state\")\n",
    "# player_table.drop_column(\"birth_city\")\n",
    "# player_table.drop_column(\"death_year\")\n",
    "\n",
    "# player_table.drop_column(\"death_month\")\n",
    "# player_table.drop_column(\"death_day\")\n",
    "# player_table.drop_column(\"death_country\")\n",
    "\n",
    "# player_table.drop_column(\"death_state\")\n",
    "# player_table.drop_column(\"death_city\")\n",
    "# player_table.drop_column(\"name_given\")\n",
    "# print(player_table.column_names)\n",
    "# path = os.path.join(\"output_data\", \"player.txt\")\n",
    "# player_table.save_to_file(path)\n",
    "\n",
    "\n",
    "# batting_fielding_joined = batting_table.perform_inner_join(fielding_table, [\"playerID\", \"yearID\"])\n",
    "# path = os.path.join(\"output_data\", \"batting_fielding_joined.txt\")\n",
    "# batting_fielding_joined.save_to_file(path)\n",
    "path = os.path.join(\"output_data\", \"batting_fielding_joined.txt\")\n",
    "batting_fielding_joined = mypytable.MyPyTable().load_from_file(path)\n",
    "# print(len(batting_fielding_joined.data))\n",
    "# path = os.path.join(\"output_data\", \"Allstar.txt\")\n",
    "# allstar_table = mypytable.MyPyTable().load_from_file(path)\n",
    "# allstar_dict = {}\n",
    "# for instance in allstar_table.data:\n",
    "#     allstar_dict[instance[0]] = instance[1]\n",
    "# for instance in batting_fielding_joined.data:\n",
    "#     if instance[0] in allstar_dict:\n",
    "#         try:\n",
    "#             if instance[1] == allstar_dict[instance[0]]:\n",
    "#                 instance.append(\"true\")\n",
    "#             else:\n",
    "#                 instance.append(\"false\")\n",
    "#         except:\n",
    "#             instance.append(\"false\")\n",
    "#     else:\n",
    "#         instance.append(\"false\")\n",
    "# batting_fielding_joined.column_names.append('class')\n",
    "# path = os.path.join(\"output_data\", \"batting_fielding_joined.txt\")\n",
    "# batting_fielding_joined.save_to_file(path)\n",
    "\n",
    "\n",
    "# count1 = 0\n",
    "# count2 = 0\n",
    "# for row in batting_fielding_joined.data:\n",
    "#     if row[-1] == \"true\":\n",
    "#         count1+=1\n",
    "#     elif row[-1] == \"false\":\n",
    "#         count2 += 1\n",
    "# print(count1, count2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Final Data Table\n",
    "\n",
    "Now, we will take the joined tables to create a simpler, smaller table joined from the batting and fielding data. This table will be used to then visualize some important statistics as well as train the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "\n",
    "importlib.reload(myutils)\n",
    "importlib.reload(mypytable)\n",
    "\n",
    "import mysklearn.plot_utils as plot_utils\n",
    "importlib.reload(plot_utils)\n",
    "\n",
    "# put data into a table\n",
    "path = os.path.join(\"output_data\", \"batting_fielding_joined.txt\")\n",
    "batting_fielding_joined = mypytable.MyPyTable().load_from_file(path)\n",
    "batting_fielding_joined.drop_column(\"playerID\")\n",
    "table_copy = copy.deepcopy(batting_fielding_joined.data)\n",
    "\n",
    "for row in table_copy:\n",
    "    for i in range(len(row)):\n",
    "        row[i] = str(row[i])\n",
    "\n",
    "# separating allstars from non-allstars to make the dataset\n",
    "rows_of_allstar = []\n",
    "rows_of_nonallstar = []\n",
    "for row in table_copy:\n",
    "    if row[-1] == \"true\":\n",
    "        rows_of_allstar.append(row)\n",
    "    else:\n",
    "        rows_of_nonallstar.append(row)\n",
    "\n",
    "# take 524 non-allstars to form even class distribution\n",
    "random.shuffle(rows_of_nonallstar)\n",
    "new_non_allstar = []\n",
    "for i in range(524):\n",
    "    new_non_allstar.append(rows_of_nonallstar[i])\n",
    "table_copy = rows_of_allstar + new_non_allstar\n",
    "random.shuffle(table_copy)\n",
    "\n",
    "final_table = mypytable.MyPyTable(column_names=batting_fielding_joined.column_names, data=table_copy)\n",
    "final_table_filename = 'final_table.txt'\n",
    "final_table_path = os.path.join('output_data', final_table_filename)\n",
    "final_table.save_to_file(filename=final_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing our Working Dataset (EDA)\n",
    "\n",
    "First, let's look at the number of players from each year in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(plot_utils)\n",
    "\n",
    "# get the number of players from each year\n",
    "year_column_name = \"yearID\"\n",
    "year_column_index = final_table.column_names.index(year_column_name)\n",
    "players_per_year_dict = dict()\n",
    "for row in final_table.data:\n",
    "    player_year = row[year_column_index]\n",
    "    if players_per_year_dict.get(player_year) is None:\n",
    "        players_per_year_dict[player_year] = 1\n",
    "    else:\n",
    "        players_per_year_dict[player_year] += 1\n",
    "\n",
    "# sort the data by year\n",
    "sorted_keys = sorted(list(players_per_year_dict.keys()))\n",
    "sorted_players_per_year_dict = dict()\n",
    "for key in sorted_keys:\n",
    "    sorted_players_per_year_dict[key] = players_per_year_dict[key]\n",
    "\n",
    "# create a bar chart from the data TODO - label the chart better\n",
    "plot_utils.plot_bar_chart(x_values=list(sorted_players_per_year_dict.keys()), y_values=list(sorted_players_per_year_dict.values()), \n",
    "    x_tick_labels=list(sorted_players_per_year_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bar chart shows us that we have slightly more data from recent years. This makes sense because as time progresses, professional athletes' stats are measured more carefully. We will keep this in mind when making predictions, as the predictions will be more accurate for players playing in more recent seasons.\n",
    "\n",
    "Now, let's take a look at the number of games played for each player in our dataset so we can make sure there is a good spread of players who have played only a few games and players who have played a lot of games. We want more players with a large number of games played so thereare more accurate statistics for them. These values have been discretized already to make this process easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(plot_utils)\n",
    "\n",
    "# get the number of players with each discretized value of games played\n",
    "games_played_column_name = \"G\"\n",
    "games_played_column_index = final_table.column_names.index(games_played_column_name)\n",
    "players_per_games_played_dict = dict()\n",
    "for row in final_table.data:\n",
    "    player_games_played = row[games_played_column_index]\n",
    "    if players_per_games_played_dict.get(player_games_played) is None:\n",
    "        players_per_games_played_dict[player_games_played] = 1\n",
    "    else:\n",
    "        players_per_games_played_dict[player_games_played] += 1\n",
    "\n",
    "# sort the data by games played\n",
    "sorted_keys = sorted(list(players_per_games_played_dict.keys()))\n",
    "sorted_players_per_games_played_dict = dict()\n",
    "for key in sorted_keys:\n",
    "    sorted_players_per_games_played_dict[key] = players_per_games_played_dict[key]\n",
    "\n",
    "# create a bar chart from the data TODO - label the chart better\n",
    "plot_utils.plot_bar_chart(x_values=list(sorted_players_per_games_played_dict.keys()), y_values=list(sorted_players_per_games_played_dict.values()), \n",
    "    x_tick_labels=list(sorted_players_per_games_played_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was not the spread of data we were expecting. Despite this, we do have the large number of players with high games played. On the other hand, there are not many players with an \"average\" number of games played in our dataset. These two observations tell us that the predictions might not be all that accurate for the average player, however the average player should not be an All Star, so this should not pose too much of an issue.\n",
    "\n",
    "### Classification Results\n",
    "\n",
    "To determine the best classifier for this dataset, we first must evaluate three of the classifiers available to us. These classifiers that will be evaluated are the Dummy Classifier, Naive Bayes Classifier, and the Random Forest Classifier. To evaluate tehm, we will use stratified k-Fold Cross Validation to obtain train and test samples, and from these samples we will measure the accuracy, recall, and f-measure of each classifier to determine which is the best for our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Dummy Classifier -------\n",
      "accuracy: 0.4990412272291467\n",
      "recall: 0.7142857142857143\n",
      "f-measure: 0.4757633995302157\n",
      "\n",
      "------- Decision Tree Classifier -------\n",
      "accuracy: 1.0\n",
      "recall: 1.0\n",
      "f-measure: 1.0\n",
      "\n",
      "------- Random Forest Classifier -------\n",
      "accuracy: 1.0\n",
      "recall: 1.0\n",
      "f-measure: 1.0\n"
     ]
    }
   ],
   "source": [
    "import mysklearn.myevaluation as myevaluation\n",
    "importlib.reload(myevaluation)\n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier, \\\n",
    "    MyDecisionTreeClassifier, MyRandomForestClassifier\n",
    "\n",
    "# create train and test datasets\n",
    "y_data = final_table.get_column('class')\n",
    "X_data = final_table.data\n",
    "num_splits = 7\n",
    "train_fold_indices, test_fold_indices = myevaluation.stratified_kfold_cross_validation(X=X_data, y=y_data, n_splits=num_splits)\n",
    "\n",
    "# evaluation stats held for each classifier (order: [accuracy, recall, f-measure])\n",
    "dummy_classifier_eval = [0, 0, 0]\n",
    "decision_tree_classifier_eval = [0, 0, 0]\n",
    "forest_classifier_eval = [0, 0, 0]\n",
    "# evaluate the classifiers\n",
    "for fold in range(len(train_fold_indices)):\n",
    "    X_train = [X_data[index] for index in train_fold_indices[fold]]\n",
    "    y_train = [y_data[index] for index in train_fold_indices[fold]]\n",
    "    X_test = [X_data[index] for index in test_fold_indices[fold]]\n",
    "    y_test = [y_data[index] for index in test_fold_indices[fold]]\n",
    "    # evaluate the dummy classifier on the current fold\n",
    "    baseball_dummy_classifier = MyDummyClassifier()\n",
    "    baseball_dummy_classifier.fit(X_train=X_train, y_train=y_train)\n",
    "    y_predicted = baseball_dummy_classifier.predict(X_test=X_test)\n",
    "    dummy_classifier_eval[0] += myevaluation.accuracy_score(y_true=y_test, y_pred=y_predicted)\n",
    "    dummy_classifier_eval[1] += myevaluation.binary_recall_score(y_true=y_test, y_pred=y_predicted)\n",
    "    dummy_classifier_eval[2] += myevaluation.binary_f1_score(y_true=y_test, y_pred=y_predicted)\n",
    "    # evaluate the Decision Tree classifier on the current fold\n",
    "    baseball_decision_tree_classifier = MyDecisionTreeClassifier()\n",
    "    baseball_decision_tree_classifier.fit(X_train=X_train, y_train=y_train)\n",
    "    y_predicted = baseball_decision_tree_classifier.predict(X_test=X_test)\n",
    "    decision_tree_classifier_eval[0] += myevaluation.accuracy_score(y_true=y_test, y_pred=y_predicted)\n",
    "    decision_tree_classifier_eval[1] += myevaluation.binary_recall_score(y_true=y_test, y_pred=y_predicted)\n",
    "    decision_tree_classifier_eval[2] += myevaluation.binary_f1_score(y_true=y_test, y_pred=y_predicted)\n",
    "    # evaluate the Random Forest classifier on the current fold\n",
    "    baseball_forest_classifier = MyRandomForestClassifier()\n",
    "    baseball_forest_classifier.fit(X_train=X_train, y_train=y_train, N=15, F=len(X_train)-25, M=8)\n",
    "    y_predicted = baseball_forest_classifier.predict(test_instances=X_test)\n",
    "    forest_classifier_eval[0] += myevaluation.accuracy_score(y_true=y_test, y_pred=y_predicted)\n",
    "    forest_classifier_eval[1] += myevaluation.binary_recall_score(y_true=y_test, y_pred=y_predicted)\n",
    "    forest_classifier_eval[2] += myevaluation.binary_f1_score(y_true=y_test, y_pred=y_predicted)\n",
    "\n",
    "\n",
    "dummy_classifier_eval[0] = dummy_classifier_eval[0] / num_splits\n",
    "dummy_classifier_eval[1] = dummy_classifier_eval[1] / num_splits\n",
    "dummy_classifier_eval[2] = dummy_classifier_eval[2] / num_splits\n",
    "print('------- Dummy Classifier -------')\n",
    "print('accuracy:', dummy_classifier_eval[0])\n",
    "print('recall:', dummy_classifier_eval[1])\n",
    "print('f-measure:', dummy_classifier_eval[2])\n",
    "decision_tree_classifier_eval[0] = decision_tree_classifier_eval[0] / num_splits\n",
    "decision_tree_classifier_eval[1] = decision_tree_classifier_eval[1] / num_splits\n",
    "decision_tree_classifier_eval[2] = decision_tree_classifier_eval[2] / num_splits\n",
    "print('\\n------- Decision Tree Classifier -------')\n",
    "print('accuracy:', decision_tree_classifier_eval[0])\n",
    "print('recall:', decision_tree_classifier_eval[1])\n",
    "print('f-measure:', decision_tree_classifier_eval[2])\n",
    "forest_classifier_eval[0] = forest_classifier_eval[0] / num_splits\n",
    "forest_classifier_eval[1] = forest_classifier_eval[1] / num_splits\n",
    "forest_classifier_eval[2] = forest_classifier_eval[2] / num_splits\n",
    "print('\\n------- Random Forest Classifier -------')\n",
    "print('accuracy:', forest_classifier_eval[0])\n",
    "print('recall:', forest_classifier_eval[1])\n",
    "print('f-measure:', forest_classifier_eval[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dummy Classifier was by far the worst. Its accuracy, recall, and f-measure were not as high as both other classifiers. The Decision Tree and Random Forest Classifiers had perfect accuracy, recall, and f-measure. This is likely because of the high correlation between specific data points and the class in the data set. This is a good thing! Ultimately, we believe that the Random Forest Classifier is the best because it is not quite as specialized as the single Decision Tree Classifier.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Overall, we are happy with the solution we implemented. We were able to work with data related to a topic we enjoy, implemented a new classifier, and created a web app to show our results.  \n",
    "\n",
    "Future work and Improvements:\n",
    "\n",
    "* We could potentially improve classification by handling missing values better. Currently we replace missing values 0 which could have a big effect on a players stats positvely or negatively. \n",
    "\n",
    "* More time could be spent on developing a cleaner looking UI for the Flask app\n",
    "\n",
    "\n",
    "\n",
    "Project Management:\n",
    "\n",
    "* Josh was responsible for implementing our final decision-tree forest classifier. He also implemented the bonus of creating a Flask app with a basic user interface. \n",
    "\n",
    "* Weston was responsible for collecting data, data cleaning, and discretizing data for classification. \n",
    "\n",
    "Dataset Source: https://www.kaggle.com/datasets/seanlahman/the-history-of-baseball?resource=download\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98cc8a3e9e9b1e128450920e7c198c9bd6e3908fe3cb16854537f0ebed56a5f5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
